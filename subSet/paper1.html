
<!-- saved from url=(0041)https://cs.stanford.edu/~kaichun/partnet/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>PartNet</title>
<link href="../css/paperstyle.css" rel="stylesheet" type="text/css">
<link rel="shortcut icon" href="pic/logo.ico">
</head>

<body>

<div class="pageTitle">
  PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding
  <br>
  <br>
  <span class="Authors">
      <a href="http://cs.stanford.edu/~kaichun" target="_blank">Kaichun Mo</a><sup>1</sup> &nbsp; &nbsp;
      <a href="http://cseweb.ucsd.edu/~shz338/" target="_blank">Shilin Zhu</a><sup>2</sup> &nbsp; &nbsp;
      <a href="https://angelxuanchang.github.io/" target="_blank">Angel X. Chang</a><sup>3</sup> &nbsp; &nbsp;
      <a href="https://cs.stanford.edu/~ericyi/" target="_blank">Li Yi</a><sup>1</sup> &nbsp; &nbsp;<br>
      <a href="https://subarnatripathi.github.io/" target="_blank">Subarna Tripathi</a><sup>4</sup> &nbsp; &nbsp;
      <a href="https://geometry.stanford.edu/member/guibas/" target="_blank">Leonidas J. Guibas</a><sup>1,5</sup> &nbsp; &nbsp;
      <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a><sup>2</sup> &nbsp; &nbsp;<br><br>
      <sup>1</sup><a href="http://www.stanford.edu/" target="_blank"> Stanford University </a> &nbsp; &nbsp;
      <sup>2</sup><a href="http://www.ucsd.edu/" target="_blank"> University of California San Diego </a> &nbsp; &nbsp;<br>
      <sup>3</sup><a href="http://www.sfu.ca/" target="_blank"> Simon Fraser University </a> &nbsp; &nbsp;
      <sup>4</sup><a href="https://ai.intel.com/" target="_blank"> Intel AI Lab </a> &nbsp; &nbsp;
      <sup>5</sup><a href="https://research.fb.com/category/facebook-ai-research/" target="_blank"> Facebook AI Research </a><br><br>
      <a href="http://cvpr2019.thecvf.com/" target="_blank"><i>Conference on Computer Vision and Pattern Recognition (CVPR) 2019</i></a>
  </span>
  </div>
<br>
<div class="material">
        <a href="https://arxiv.org/abs/1812.02713" target="_blank">[ArXiv Preprint (Low-res)]</a>
        <a href="https://cs.stanford.edu/~kaichun/partnet/poster.pdf" target="_blank">[Poster]</a>
        <a href="https://cs.stanford.edu/~kaichun/partnet/paper.bib" target="_blank">[BibTex]</a> 
        <a href="https://github.com/daerduoCarey/partnet_dataset" target="_blank">[Code (Github)]</a><br>
        <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Mo_PartNet_A_Large-Scale_Benchmark_for_Fine-Grained_and_Hierarchical_Part-Level_3D_CVPR_2019_paper.pdf" target="_blank">[Paper (High Res)]</a>
        <a href="https://cs.stanford.edu/~kaichun/partnet/partnet_supp_high_res.pdf" target="_blank">[Supplementary Materials (High Res)]</a><br>
        <a href="https://www.shapenet.org/download/parts" target="_blank">[Pre-release v0]</a> 
        <a href="https://cs.stanford.edu/~kaichun/partnet/benchmark.txt" target="_blank">[Online Benchmark (Stay Tuned)]</a> 
</div>

  <p class="abstractText">
  <b style="color: green; background-color: #ffff42">NEW</b> [April 1, 2020] We release the pretrained models for PointCNN part semantic segmentation <a href="http://download.cs.stanford.edu/orion/partnet_dataset/pointcnn_sem_seg_pretrained_ckpts.zip" target="_blank">here</a> (4.5G). We are working on a ScanNet-like online benchmark to release by June 2020!</p>
  <p class="abstractText">
  [Oct 5, 2019] The authors of <a href="https://kevinkaixu.net/projects/partnet.html" target="_blank">this paper</a> enrich our PartNet dataset with their binary symmetry hierarchies. Please refer to <a href="https://github.com/kevin-kaixu/partnet-symh" target="_blank">this link</a> to download the data. Please cite both our paper and their paper if you use this data. <b>Disclaimer: </b> This is a separate effort done by Nanjing University and National University of Defense Technology (NUDT). Please directly contact <a href="mailto:kevin.kai.xu@gmail.com" target="_blank">Kevin Xu</a> for questions and instructions. As part of our <a href="https://cs.stanford.edu/~kaichun/structurenet/" target="_blank">StructureNet</a> paper, we release n-ary part hierarchies for PartNet models. </p>
  <p class="abstractText">
  [July 11, 2019] We release the pretrained models for the proposed part instance segmentation method <a href="http://download.cs.stanford.edu/orion/partnet_dataset/ins_seg_ours_pretrained_models.zip" target="_blank">here</a> (1.3G). </p>
  <p class="abstractText">
  [April 19, 2019] We release our 3D web-based annotation system <a href="https://github.com/daerduoCarey/partnet_anno_system" target="_blank">here</a>. Please watch <a href="https://youtu.be/7pEuoxmb-MI" target="_blank">this video</a> for reference.</p>
  <p class="abstractText">
   [March 29, 2019] Updated prelease of PartNet v0 with better organization, visualizations, and separate downloads for HDF5 file sfor semantic and instance segmentation tasks. Github repo and Data download page are updated.</p>
  <p class="abstractText">
  [March 12, 2019] We release <a href="https://www.shapenet.org/download/parts" target="_blank">PartNet v0 (pre-release version)</a> as a part of ShapeNet effort. Please register as a ShapeNet user first in order to download. We provide meta-files and scripts on <a href="https://github.com/daerduoCarey/partnet_dataset" target="_black">this Github repo</a>. If you have any questions, please post <a href="https://github.com/daerduoCarey/partnet_dataset/issues" target="_blank">issues on the github page</a>. If you have general feedbacks, please fill in <a href="https://docs.google.com/forms/d/e/1FAIpQLSetsP7aj-Hy0gvP2FxRT3aTIrc_IMqSqR-5Xl8P3x2awDkQbw/viewform?usp=sf_link" target="_blaank">this form</a> to let us know. If you observe any data annotation error, please fill in <a href="https://docs.google.com/spreadsheets/d/1Q_6r9EblZdP9Grhhm2ob4u0FQ8xurAThlgK-qAcjYP0/edit#gid=0" target="_blank">this errata</a> to help improve PartNet. We will release stable version v1 later and will host PartNet challenges on v1.
</p>


  <div class="abstractTitle">
  Abstract
  </div>
  <p class="abstractText">
  We present PartNet: a <i>consistent</i>, <i>large-scale</i> dataset of 3D objects annotated with <i>fine-grained</i>, <i>instance-level</i>, and <i>hierarchical</i> 3D part information. Our dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others. Using our dataset, we establish three benchmarking tasks for evaluating 3D part recognition: <i>fine-grained semantic segmentation</i>, <i>hierarchical semantic segmentation</i>, and <i>instance segmentation</i>. We benchmark four state-of-the-art 3D deep learning algorithms for fine-grained semantic segmentation and three baseline methods for hierarchical semantic segmentation. We also propose <i>a novel method for part instance segmentation</i> and demonstrate its superior performance over existing methods.
</p>

<div class="abstractTitle">
    Dataset Visualization
</div>
  <img class="bannerImage" src="./PartNet_files/teaser.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
              Figure 1. <b>PartNet Dataset and Three Benchmarking Tasks. </b>
              Left: we show example annotations at three levels of segmentation in the hierarchy. Right: we propose three fundamental and challenging segmentation tasks and establish benchmarks using PartNet.
  </p></td></tr></tbody></table>

  <img class="bannerImage" src="./PartNet_files/data_visu.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
              Figure 2. <b>PartNet Fine-grained Instance-level Shape Part Annotation.</b>
              We visualize example shapes with fine-grained part annotations for the 24 object categories in PartNet.
  </p></td></tr></tbody></table>

  <img class="bannerImage" src="./PartNet_files/lamp_tree.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
              Figure 3. <b>PartNet Hierarchical Shape Part Template and Annotation.</b>
We show the expert-defined hierarchical template for lamp (middle) and the instantiations for a table lamp (left) and a ceiling lamp(right). The And-nodes are drawn in solid lines and Or-nodes in dash lines. The template is deep and comprehensive to cover structurallydifferent types of lamps. In the meantime, the same part concepts, such as light bulb and lamp shade, are shared across the different types.
  </p></td></tr></tbody></table>

  <div class="abstractTitle">
    Data Annotation
</div>
<img class="bannerImage" src="./PartNet_files/interface.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
              Figure 4. <b>Data Annotation Workflow.</b>
              We show our annotation interface with its components, the proposed question-answering workflow and the mesh cutting interface.
  </p></td></tr></tbody></table>

  <div class="abstractTitle">
  Video
  </div>
  <center>
      <iframe width="560" height="315" src="./PartNet_files/7pEuoxmb-MI.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
  </center>

  <div class="abstractTitle">
    A Novel Method for Part Instance Segmentation
</div>
<img class="bannerImage" src="./PartNet_files/ins_seg_arch.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
              Figure 5. <b>The Proposed Method for Part Instance Segmentation.</b> The network learns to predict three components: the semantic label for each point, a set of disjoint instance masks and their confidence scores for part instances.
  </p></td></tr></tbody></table>

  <img class="bannerImage" src="./PartNet_files/ins_seg_res1.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
              Figure 6. <b>Qualitative Comparison To SGPN.</b> 
              We show qualitative comparisons for our proposed method and SGPN.
              Our method produces more robust and cleaner results than SGPN.

  </p></td></tr></tbody></table>

<img class="bannerImage" src="./PartNet_files/ins_seg_res2.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
              Figure 7. <b>Learned Part Instance Correspondences.</b>
              Our proposed method learns structural priors within each object category that is more instance-aware and robust in predicting complete instances. 
              We observe that training for a set of disjoint masks across multiple shapes gives us consistent part instances.
              The corresponding parts are marked with the same color.
  </p></td></tr></tbody></table>



  <div class="abstractTitle">
  Acknowledgements
  </div>
  <p class="abstractText">
  This research was supported by NSF grants CRI-1729205, IIS-1763268, and CHS-1528025, a Vannevar Bush Faculty Fellowship, a Google fellowship, and gifts from Autodesk, Google and Intel AI Lab.
  We especially thank Zhe Hu from Hikvision for the help on data annotation and Linfeng Zhao for the help on preparing hierarchical templates.
  We appreciate the 66 annotators from Hikvision, Ytuuu and Data++ on data annotation.
  We also thank Ellen Blaine for helping with the narration of the video.
</p>



<p></p>

</body></html>