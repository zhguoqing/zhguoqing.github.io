<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Guoqing Zhang, 张国庆, Person Re-identification, Domain Adaptation Learning, Deep Learning, Computer Vision, Face Recognition, Nanyang Technological University">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./Files/favicon.ico">
<title>Guoqing Zhang</title>
</head>
 
 
<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable">
 <tr>
  <td>
<a ><img src="./Files/guoqing.JPG" alt="" height="225px" /></a>&nbsp;
  </td>
<td align="left"><p><font size="4">Guoqing Zhang (</font><font size="4"; font style="font-family:Microsoft YaHei">张国庆</font><font size="4">)</font><br />
<!-- <i> Assistant Professor</i> -->
<br />
I have joint Nanjing University of Information Science and Technology since Jul 2017. I received my Ph.D. degree from Nanjing University of Science and Technology with 
 Prof. <a href="./">Huaijiang Sun</a>. I have been a visiting student working with Prof. <a href="http://www.porikli.com/">Fatih Porikli</a> at the Australian National 
 University (ANU) from Oct. 2015 to Oct. 2016. After 2018, I am a Postdoctoral Researcher at the School of Computer Science and Engineering, Nanyang Technological University 
 (NTU), Singapore, worked with Prof. <a href="https://www.ntu.edu.sg/home/wslin/">Weisi Lin</a>.<br />
<br />
Location: School of Computer and Software, Nanjing University of Information Science and Technology, Jiangsu, China<br />
<class="staffshortcut">
 <A HREF="#Interest">Research Interest</A> | 
 <A HREF="#News">News</A> | 
 <A HREF="#Education">Education</A> | 
 <A HREF="#Publications">Publications</A> | 
 <A HREF="#Projects">Projects</A> | 
 <A HREF="#Services">Services</A> | 
 <A HREF="#Awards">Awards</A>
<br />
<br />
 
Email: xiayang14551@163.com; &nbsp;&nbsp; guoqing.zhang@ntu.edu.sg <br />
[<a href="https://scholar.google.com.sg/citations?user=z14O5OIAAAAJ&hl=en" target="_blank">Google Scholar</a>]
[<a href="https://github.com/zhguoqing" target="_blank">GitHub</a>]
<!-- [<a href="https://www.researchgate.net/profile/Qiang_Zhang204" target="_blank">ResearchGate</a>] -->
<!-- [<a href="https://orcid.org/0000-0002-7116-9327" target="_blank">ORCID</a>]  -->
<!-- [<a href="./Files/weixin.png" target="_blank"><font style="font-family:Microsoft YaHei">微信</font></a>] -->
</td>
 </tr>
 </table>


<A NAME="Interest"><h2>Research Interest</h2></A>
My research interest includes Computer Vision, Pattern Recognition and Deep learning theory. Currently, I focus on the following research topics:
<ul>
<li>Person Re-identification</li>
<Li>Domain Adaptation Learning</Li>
<li>Dictionary Learning</li>
<li>Feature Extraction</li>
</ul>
<br />
 
 

<A NAME="News"><h2>News</h2></A>
<ul>
<!-- <li> <b> <font color="#FF0000">[2020.10]</font> </b> Our <b>Multi-Temporal Full Width Sentinel-2 Cloud Detection and Removal Dataset</b> is released 
 (<a href= "https://drive.google.com/file/d/1LlvUKtUWAKoF6R0igbREwvP2Wfja9UBv/view?usp=sharing" target="_blank">dataset</a>)!</li> -->
<!-- <li> <b> <font color="#FF0000">[2020.09]</font> </b> Awarded with <b>Graduate Academic Innovation Outstanding Prize</b> 
 (<a href="https://www.gs.whu.edu.cn/info/1057/7261.htm" target="_blank"><font style="font-family:Microsoft YaHei">武汉大学研究生学术创新校长奖</font></a>)!</li> -->
<!-- <li> <b> <font color="#FF0000">[2020.08]</font> </b> Our <b>SGD-SM Productions</b> are released 
 (<a href= "https://github.com/qzhang95/SGD-SM" target="_blank">code</a>, <a href= "https://qzhang95.github.io/Projects/Global-Daily-Seamless-AMSR2/" target="_blank">dataset</a>)!</li> -->
<!-- <li> <b> <font color="#FF0000">[2020.06]</font> </b> Our <b>HSI Denoising Dataset</b> is released 
 (<a href= "https://wws.lanzous.com/iNjl9dna1vi" target="_blank">dataset</a>)!</li> -->
<!-- <li> <b> <font color="#FF0000">[2020.05]</font> </b> Invited as a reporter for <b>GeoScience Café</b> 
 (<a href= "./Files/张强_Geoscience Cafe254期.pdf" target="_blank">slides</a>)!</li> -->
 
<li> <b> <font color="#FF0000">[2022.07]</font> </b> One corresponding authored paper was accepted by <b>IEEE TCSVT</b> </li>
<li> <b> <font color="#FF0000">[2022.07]</font> </b> I was invited to be a <b>Session Chair for ICME 2022</b> </li>
<li> <b> <font color="#FF0000">[2022.06]</font> </b> One paper was accepted by <b>JVCIR</b> </li>
<li> <b> <font color="#FF0000">[2022.04]</font> </b> One paper was accepted by <b>IEEE TCSVT</b> </li>
<li> <b> <font color="#FF0000">[2022.04]</font> </b> One paper was accepted by <b>Neurocomputing</b> </li>
<li> <b> <font color="#FF0000">[2022.03]</font> </b> One paper was accepted by <b>ICME 2022</b> </li>
<li> <b> <font color="#FF0000">[2022.02]</font> </b> One paper was accepted by <b>Neurocomputing</b> </li>
<li> <b> <font color="#FF0000">[2021.09]</font> </b> One paper was accepted by <b>IEEE Transactions on Image Processing</b> </li>
<li> <b> <font color="#FF0000">[2021.07]</font> </b> One paper was accepted by <b>Information Sciences</b> </li>
<li> <b> <font color="#FF0000">[2021.04]</font> </b> One conference paper is accepted by <b>IJCAI 2021 (13.9% acceptance rate)</b> </li>
<li> <b> <font color="#FF0000">[2021.03]</font> </b> One conference paper is accepted by <b>ICME 2021</b>, Congratulations Yuhao</li>
<li> <b> <font color="#FF0000">[2021.02]</font> </b> One paper is accepted by <b>Multimedia Tools and Applications</b></li>
</ul>
<br />




 
<A NAME="Education"><h2>Education</h2></A>
<ul>
 <li>2018.10-2022.03 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Postdoctoral in School of Computer Science and Engineering, Nanyang Technological University (NTU). 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="https://www.ntu.edu.sg/home/wslin/" target="_blank">Weisi Lin</a></li>
 <Li>2015.10-2016.11 &nbsp;&nbsp; Visiting Student in College of Engineering and Computer Science, The Australian National University (ANU). &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="http://www.porikli.com/" target="_blank">Fatih Porikli</a></li>
<li>2012.9-2017.6   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ph.D in School of Computer Science and Engineering, Nanjing University of Science and Technology (NUST). &nbsp;Supervisor: Prof. Huaijiang Sun</li>
</ul>
<br />

 


<A NAME="Publications"><h2>Publications</h2></A>
<p><b>Journals</b>: </p>
<font size="3"> 
<ul>

<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[22] <b>G. Zhang</b>, Z. Luo, Y. Chen, Y. Zheng, and W. Lin, 
“Fine-grained-based Multi-feature Fusion for Occluded Person Re-identification,” 
<i>Journal of Visual Communication and Image Representation(<b>JVCIR</b>)</i>, 
vol. 87, 2022. 
(<b>SCI Q3, IF=2.887</b>)
</span>
</p>
 
 
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[21] <b>G. Zhang</b>, Z. Luo, Y. Chen, Y. Zheng, and W. Lin, 
“Illumination Unification for Person Re-identification,” 
<i>IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>)</i>, 
DOI: 10.1109/TCSVT.2022.3169422, 2022. 
(<b>SCI Q1, IF=4.685</b>)
[<a href= "./Files/2019_TCSVT_ODPDL.pdf" target="_blank">PDF</a>]
<!-- [<a href= "https://wws.lanzous.com/iNjl9dna1vi" target="_blank">Dataset</a>] -->
</span>
</p>
 
 <p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[20] Y. Chen, <b>G. Zhang*</b>, Y. Lu, Z. Wang, and Y. Zheng, 
“A Simple but Effective Part-based Convolutional Baseline for Text-based Person Search,” 
<i>Neurocomputing</i>, 
vol. 494, pp. 171-181, 2022.
(<b>SCI Q2, IF=5.719</b>)
</span>
</p>
 
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[19] <b>G. Zhang</b>, H. Zhang, Y. Chen, and Y. Zheng, 
“Close-set Camera Style Distribution Alignment for Single Camera Person Re-Identification,” 
<i>Neurocomputing</i>, 
vol. 486, no. 14, pp. 93-103, 2022.
(<b>SCI Q2, IF=5.719</b>)
</span>
</p>
 
 
 
 
 <p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[18] <b>G. Zhang</b>, Y. Ge, Z. Dong, H. Wang, Y. Zheng, and S. Chen, 
“Deep High-Resolution Representation Learning for Cross-Resolution Person Re-identification,” 
<i>IEEE Transactions on Image Processing(<b>TIP</b>)</i>, 
vol. 30, pp. 8913-8925, 2021.
(<b>SCI Q1 Top, IF=10.856</b>)
</span>
</p>
 
 <p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[17] <b>G. Zhang</b>, J. Yang, Y. Zheng, Y. Wang, Y. Wu, and S. Chen, 
“Hybrid-Attention Guided Network with Multiple Resolution Features for Person Re-Identification,” 
<i>Information Sciences (<b>INS</b>)</i>, 
2021.
(<b>SCI Q1 Top, IF=5.91</b>)
</span>
</p>
 
 <p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[16] <b>G. Zhang</b>, T. Jiang, J. Yang, J. Xu, and Y. Zheng, 
“Cross-view Kernel Collaborative Representation Classification for Person Re-identification,” 
<i>Multimedia Tools and Applications (<b>MTAP</b>)</i>, 
vol. 80, no. 13, 20687-20705, 2021.  
(<b>SCI Q3, IF=2.313</b>)
<!-- [<a href= "https://wws.lanzous.com/iNjl9dna1vi" target="_blank">Dataset</a>] -->
</span>
</p>
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[15] <b>G. Zhang</b>, J. Yang, Y. Zheng, Z. Luo, and J. Zhang, 
“Optimal Discriminative Feature and Dictionary Learning for Image Set Classification,” 
<i>Information Sciences (<b>INS</b>)</i>, 
vol. 574, no. 8, pp. 498-513, 2021. 
(<b>SCI Q1 Top, IF=5.91</b>)
</span>
</p>

<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[14] <b>G. Zhang</b>, F. Porikli, H. Sun, Q. Sun, and G. Xia, 
“Cost-sensitive Joint Feature and Dictionary Learning for Face Recognition,” 
<i>Neurocomputing</i>, 
vol. 391, no. 28, pp. 177-188, 2020. 
(<b>SCI Q2, IF=4.438</b>) 
[<a href= "https://www.sciencedirect.com/science/article/abs/pii/S0925231220301594" target="_blank">PDF</a>] 
<!-- [<a href="https://github.com/qzhang95/PSTCR" target="_blank">Code</a>] -->
<!-- [<a href= "https://drive.google.com/file/d/1LlvUKtUWAKoF6R0igbREwvP2Wfja9UBv/view?usp=sharing" target="_blank">Dataset</a>] -->
</span>
</p>

<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[13] <b>G. Zhang</b>, H. Sun, Y. Zheng, G. Xia, L. Feng, and Q. Sun, 
“Optimal Discriminative Projection for Sparse Representation-based Classification via Bilevel Optimization,” 
<i>IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>)</i>, 
vol. 30, no. 4, pp. 1065-1077, 2019. 
(<b>SCI Q1, IF=4.685</b>)
[<a href= "./Files/2019_TCSVT_ODPDL.pdf" target="_blank">PDF</a>]
<!-- [<a href= "https://wws.lanzous.com/iNjl9dna1vi" target="_blank">Dataset</a>] -->
</span>
</p>

<p style="text-indent: -1.6rem;margin-left: 0rem;">
 <span>[12] Y. Zheng, X. Wang, <b>G. Zhang*</b>, B. Xiao, F. Xiao, and J. Zhang, 
“Multiple kernel Coupled Projections for Domain Adaptive Dictionary Learning,” 
<i>IEEE Transactions on Multimedia (<b>TMM</b>)</i>, 
vol. 21, no. 9, pp. 2292-2304, 2019. 
(<b>SCI Q1 Top, IF=6.051 </b>)
[<a href= "./Files/2019_TMM_MK-DADP.pdf" target="_blank">PDF</a>] 
<!-- [<a href="https://github.com/qzhang95/STS-CNN" target="_blank">Code</a>] -->
</span>
</p>

<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[11] <b>G. Zhang</b>, Y. Zheng, and G. Xia, 
“Domain Adaptive Collaborative Representation based Classification,” 
<i>Multimedia Tools and Applications (<b>MTAP</b>)</i>, 
vol. 78, no. 21, 30175-30196, 2019. 
(<b>SCI Q3, IF=2.313</b>)
<!-- [<a href= "qzhang95.github.io/Files/RS_2018_SAR-DRN.pdf" target="_blank">PDF</a>] -->
</span>
</p>

<p style="text-indent: -1.6rem;margin-left: 0rem;">
 <span>[10] G. Xia, H. Sun, B. Chen, Q. Liu, L. Feng, <b>G. Zhang</b>, and R. Hang,
“Nonlinear low-rank matrix completion for human motion recovery,” 
<i>IEEE Transactions on Image Processing (<b>TIP</b>)</i>, 
vol. 27, no. 6, pp. 3011-3024, 2018. 
(<b>SCI Q1 Top, IF=10.856</b>)  
<!-- (<b>SCI Q1 Top, IF=9.34, <font color="#FF0000">ESI Highly Cited Paper</font></b>) -->
<!-- [<a href= "./Files/TGRS_2019_HSID-CNN.pdf" target="_blank">PDF</a>] -->
<!--  [<a href="https://github.com/qzhang95/HSID-CNN" target="_blank">Code</a>] -->
<!--  [<a href= "https://wws.lanzous.com/iNjl9dna1vi" target="_blank">Dataset</a>] -->
</span>
</p>
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[9] <b>G. Zhang</b>, H. Sun, F. Porikli, Y. Liu, and Q. Sun, 
“Optimal Couple Projections for Domain Adaptive Sparse Representation-based Classification,” 
<i>IEEE Transactions on Image Processing (<b>TIP</b>)</i>, 
vol. 26, no. 13, 5922-5935, 2017. 
(<b>SCI Q1, Top, IF=10.856</b>)
[<a href= "./Files/2017_TIP_OCPD-SRC.pdf" target="_blank">PDF</a>]
</span>
</p>
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
 <span>[8] G. Xia, H. Sun, X. Niu, <b>G. Zhang</b>, and L. Feng, 
“Keyframe extraction for human motion capture data based on joint kernel sparse representation,” 
<i>IEEE Transactions on Industrial Electronic (<b>TIE</b>)</i>, 
vol. 64, no. 2, 1589-1599, 2017. 
(<b>SCI Q1, Top, IF=7.515</b>)
<!-- [<a href= "qzhang95.github.io/Files/RS_2018_SAR-DRN.pdf" target="_blank">PDF</a>] -->
</span>
</p> 
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
 <span>[7] G. Xia, H. Sun, L. Feng, <b>G. Zhang</b>, and Y. Liu, 
“Human Motion Segmentation via Robust Kernel Sparse Subspace Clustering,” 
<i>IEEE Transactions on Image Processing (<b>TIP</b>)</i>, 
vol. 27, no. 1, 135-150, 2017. 
(<b>SCI Q1, Top, IF=10.856</b>)
<!-- [<a href= "qzhang95.github.io/Files/RS_2018_SAR-DRN.pdf" target="_blank">PDF</a>] -->
</span>
</p> 
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
 <span>[6] <b>G. Zhang</b>, H. Sun, G. Xia, and Q. Sun, 
“Multiple kernel sparse representation based orthogonal discriminative projection and its cost-sensitive extension,” 
<i>IEEE Transactions on Image Processing (<b>TIP</b>)</i>, 
vol. 25, no. 9, 4271-4285, 2016. 
(<b>SCI Q1, Top, IF=10.856</b>)
<!-- [<a href= "qzhang95.github.io/Files/RS_2018_SAR-DRN.pdf" target="_blank">PDF</a>] -->
</span>
</p>  
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
 <span>[5] G. Xia, H. Sun, <b>G. Zhang</b>, and L. Feng, 
“Human motion recovery jointly utilizing statistical and kinematic information,” 
<i>Information Science (<b>IS</b>)</i>, 
vol. 339, 189-205, 2016. 
(<b>SCI Q1, Top, IF=5.91</b>)
<!-- [<a href= "qzhang95.github.io/Files/RS_2018_SAR-DRN.pdf" target="_blank">PDF</a>] -->
</span>
</p>  
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
 <span>[4] <b>G. Zhang</b>, H. Sun, Z. Ji, Y. Yuan, and Q. Sun, 
“Cost-sensitive dictionary learning for face recognition,” 
<i>Pattern Recognition (<b>PR</b>)</i>, 
vol. 60, 613-629, 2016. 
(<b>SCI Q1, Top, IF=7.196</b>)
<!-- [<a href= "qzhang95.github.io/Files/RS_2018_SAR-DRN.pdf" target="_blank">PDF</a>] -->
</span>
</p> 

 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
 <span>[3] <b>G. Zhang</b>, H. Sun, G. Xia, and Q. Sun, 
“Kernel collaborative representation based dictionary learning and discriminative projection,” 
<i>Neurocomputing </i>, 
vol. 207, 300-309, 2016. 
(<b>SCI Q2, IF=5.719</b>)
<!-- [<a href= "qzhang95.github.io/Files/RS_2018_SAR-DRN.pdf" target="_blank">PDF</a>] -->
</span>
</p>  
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
 <span>[2] <b>G. Zhang</b>, H. Sun, G. Xia, and Q. Sun, 
“Kernel dictionary learning based discriminant analysis,” 
<i>Journal of Visual Communication and Image Representation (<b>JVCIR</b>)</i>, 
vol. 40, 470-484, 2016. 
(<b>SCI Q3, IF=2.479</b>)
<!-- [<a href= "qzhang95.github.io/Files/RS_2018_SAR-DRN.pdf" target="_blank">PDF</a>] -->
</span>
</p>   
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
 <span>[1] <b>G. Zhang</b>, H. Sun, Z. Ji, and Q. Sun, 
“Label Propagation based on collaborative representation for face recognition,” 
<i>Neurocomputing </i>, 
vol. 171, 1193-1024, 2016. 
(<b>SCI Q2, IF=5.719</b>)
<!-- [<a href= "qzhang95.github.io/Files/RS_2018_SAR-DRN.pdf" target="_blank">PDF</a>] -->
</span>
</p> 
 
 
 
</ul>
</font>
<br />
<br />
<br />

 
 
<p><b>Conferences</b>: </p>
<font size="3"> 
<ul>
<!-- <p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[3] <b>Q. Zhang</b>, F. Sun, Q. Yuan, J. Li, H. Shen, and L. Zhang, 
“Combined the data-driven with model-driven stragegy: A novel framework for mixed noise removal in hyperspectral image,” 
<i>Proceeding of the IEEE International Geoscience and Remote Sensing Symposium (<b>IGARSS</b>)</i>, 
in Hawaii, USA, 2020. 
(<b>EI, <font color="#FF0000">Oral</font></b>) 
[<a href=".Files/Qiang Zhang_Slides_IGARSS2020.pdf" target="_blank">Slides</a>]
</span>
</p> -->

 <p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[3] <b>G. Zhang</b>, J. Liu, Y. Chen, Y. Zheng, and H. Zhang, 
“Multi-biometric Unified Network for Cloth-changing Person Re-Identification,” 
<i>Proceeding of the IEEE International Conference on Multimedia and Expo (<b>ICME</b>)</i>, 
in Taibei, China, 2022. 
(<b>CCF-B</b>)
<!--[<a href="./Files/Qiang Zhang_Slides_IGARSS2018.pdf" target="_blank">Slides</a>] -->
</span>
</p>
 
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[2] <b>G. Zhang</b>, Y. Chen, W. Lin*, A. K. Chandran, and J. Xuan, 
“Low Resolution Information Also Matters: Learning Multi-Resolution Representation for Person Re-identification,” 
<i>Proceeding of the International Joint Conference on Ariticial Intelligence (<b>IJCAI</b>)</i>, 
in Montreal, Canada, 2021. 
(<b>CCF-A</b>)  
<!--[<a href="./Files/Qiang Zhang_Slides_IGARSS2019.pdf" target="_blank">Slides</a>]-->
</span>
</p>


<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[1] <b>G. Zhang</b>, Y. Chen, Y. Dai, Y. Zheng, and Y. Wu, 
“Reference-Aided Part-Aligned Feature Disentangling for Video Person Re-identification,” 
<i>Proceeding of the IEEE International Conference on Multimedia and Expo (<b>ICME</b>)</i>, 
in Shenzhen, China, 2021. 
(<b>CCF-B, <font color="#FF0000">Poster</font></b>)
<!--[<a href="./Files/Qiang Zhang_Slides_IGARSS2018.pdf" target="_blank">Slides</a>] -->
</span>
</p>
</ul>
<br />
 
 
 
 

<A NAME="Projects"><h2>Projects</h2></A>
<font size="3"> 
<ul>
<li>Intelligent Visual Analysis and Management for Airports Based on Human Attention Modelling and Multi-Task Joint Optimization</li> 
</ul>
</font>
<br />




 
<A NAME="Services"><h2>Services</h2></A>

<p><b>Membership</b>: </p>
<font size="3"> 
<ul>
<li>IEEE Member</li>
<li>JSAI Member (江苏省人工智能学会会员)</li>
</ul>
</font>
<br />
<br />
<br />
 
 
<p><b>Journal Reviewer</b>: </p>
<font size="3"> 
<ul>
<li>IEEE Transacctions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
<li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
<li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
<li>IEEE Transactions on Information Forensics and Security (TIFS)</li>
<li>IEEE Transacctions on Image Processing (TIP)</li>
<li>International Journal of Computer Vision (IJCV)</li>
<li>IEEE Access</li>
<li>Neurocomputing</li>
<li>Pattern Recognition</li>
<li>Information Sciences</li>
<li>Journal of Visual Communication and Image Representation</li>
</ul>
</font>
<br />


<!-- awards -->
<A NAME="Awards"><h2>Awards</h2></A>
<font size="3"> 
<ul>
<!-- <li>2020, Graduate Academic Innovation Outstanding Prize, Wuhan University | <font style="font-family:Microsoft YaHei">武汉大学“研究生学术创新奖”特等奖 (校长奖)</font></li> -->
<!-- <li>2019, Top-Ten Graduate Inspirational Star, Wuhan University | <font style="font-family:Microsoft YaHei">武汉大学研究生“十大励志之星”</font></li> -->
<!-- <li>2019, Guanghua Scholarship, Wuhan University | <font style="font-family:Microsoft YaHei">光华奖学金</font></li> -->
<!-- <li>2018, National Scholarship for Graduate Student, Ministry of Education | <font style="font-family:Microsoft YaHei">研究生国家奖学金</font></li> -->
<!-- <li>2018, First Prize of Academic Scholarship, Wuhan University | <font style="font-family:Microsoft YaHei">一等学业奖学金</font></li> -->
<!-- <li>2018, "Yaoqun" Academic Star, School of Geodesy and Geomatics | <font style="font-family:Microsoft YaHei">测绘学院“乐群”学术之星</font></li> -->
<!-- <li>2017, Outstanding Undergraduate, Wuhan University | <font style="font-family:Microsoft YaHei">武汉大学优秀本科毕业生</font></li> -->
<!-- <li>2015, National Encouragement Scholarship, Ministry of Education | <font style="font-family:Microsoft YaHei">国家励志奖学金</font></li> -->
</ul>
</font>
 
<br />
<br />


<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5x3ebj080sx&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
 

<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script>
$(function(){

    $(window).scroll(function(){  //If scroll

        var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll

        if( scrollt >400 ){  

            $("#back_top").fadeIn(400); 

        }else{

            $("#back_top").stop().fadeOut(400);

        }

    });

    $("#back_top").click(function(){ 

        $("html,body").animate({scrollTop:"0px"}, 200);

    }); 

});
</script>


<!--
All Rights Reserved by Guoqing Zhang. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
-->

<!--
<font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2020.10.05</p>
</font>
-->

</body>
</html>
